{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('A3/train.csv', index_col = 'ArticleId')\n",
    "X = df['Text']\n",
    "df['Category'].replace(['tech', 'entertainment'], [0,1], inplace = True)\n",
    "Y = df['Category']\n",
    "\n",
    "df_test = pd.read_csv('A3/test.csv', index_col = 'ArticleId')\n",
    "X_test = df_test['Text']\n",
    "df_test['Category'].replace(['tech', 'entertainment'], [0,1], inplace = True)\n",
    "Y_test = df_test['Category']\n",
    "\n",
    "temp_complete_test = pd.concat([X, X_test])\n",
    "\n",
    "#print(df)\n",
    "#print(df_test)\n",
    "#print(temp_complete_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency count for dataset\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(temp_complete_test)\n",
    "\n",
    "# Encode document\n",
    "vector = vectorizer.transform(temp_complete_test)\n",
    "prep_X_count = pd.DataFrame(vector.toarray(), columns = sorted(vectorizer.vocabulary_), index = temp_complete_test.index)\n",
    "#print(prep_X_count)\n",
    "\n",
    "prep_X_train_count = prep_X_count[:428]\n",
    "prep_X_test_count = prep_X_count[428:]\n",
    "\n",
    "print(prep_X_train_count)\n",
    "#print(prep_X_test_count)\n",
    "\n",
    "train_sum = prep_X_train_count.sum().sort_values(ascending = False)\n",
    "test_sum = prep_X_test_count.sum().sort_values(ascending=False)\n",
    "\n",
    "#print(train_sum)\n",
    "#print(test_sum)\n",
    "\n",
    "train_top_50 = train_sum.head(50)\n",
    "#print(train_top_50)\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(rc = {'figure.figsize':(20,8)})\n",
    "plot = sns.barplot(x=train_top_50.index, y=train_top_50.values)\n",
    "plot.set_xlabel('Top 50 words')\n",
    "plot.set_ylabel('Word count')\n",
    "plot.set_title('Term frequency in training set')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_group = Y.groupby(Y) #Group response class label\n",
    "train_tech, train_entertain = train_label_group.get_group(0).index, train_label_group.get_group(1).index\n",
    "\n",
    "#print(train_tech)\n",
    "#print(train_entertain)\n",
    "\n",
    "train_tech_sum = prep_X_count.loc[train_tech].sum().sort_values(ascending = False)\n",
    "train_entertain_sum = prep_X_count.loc[train_entertain].sum().sort_values(ascending=False)\n",
    "\n",
    "#print(train_tech_sum)\n",
    "#print(train_entertain_sum)\n",
    "\n",
    "train_top_50_tech = train_tech_sum.head(50)\n",
    "train_top_50_entertain = train_entertain_sum.head(50)\n",
    "\n",
    "#print((train_top_50_entertain))\n",
    "#print((train_top_50_entertain))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(25, 8))\n",
    "ax[0] = sns.barplot(x=train_top_50_tech.index, y=train_top_50_tech.values, ax=ax[0])\n",
    "ax[0].set_xlabel('Top 50 words')\n",
    "ax[0].set_ylabel('Word count')\n",
    "ax[0].set_title('Term frequency in tech category')\n",
    "ax[0].tick_params(labelrotation=90)\n",
    "\n",
    "ax[1] = sns.barplot(x=train_top_50_entertain.index, y=train_top_50_entertain.values, ax=ax[1])\n",
    "ax[1].set_xlabel('Top 50 words')\n",
    "ax[1].set_ylabel('Word count')\n",
    "ax[1].set_title('Term frequency in entertainment category')\n",
    "ax[1].tick_params(labelrotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(10,5)})\n",
    "#print(train_tech.size)\n",
    "#print(train_entertain.size)\n",
    "plot = sns.barplot(x=['Tech', 'Entertainment'], y=[train_tech.size, train_entertain.size])\n",
    "plot.set_xlabel('Articles')\n",
    "plot.set_ylabel('Article count')\n",
    "plot.set_title('Class distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "# summarize\n",
    "print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(X)\n",
    "\"\"\" print(f'features\\n {vectorizer.get_feature_names_out()}\\n')\n",
    "# summarize encoded vector\n",
    "print(f'vector shape: {vector.shape}\\n')\n",
    "print(f'article vector\\n {vector.toarray()}') \"\"\"\n",
    "\n",
    "prep_X = pd.DataFrame(vector.toarray(), columns=sorted(vectorizer.vocabulary_), index=X.index)\n",
    "print(prep_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(temp_complete_test)\n",
    "# summarize\n",
    "# print(f'vector vocabulary - {vectorizer.vocabulary_}\\n')\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(temp_complete_test)\n",
    "\"\"\" print(f'features\\n {vectorizer.get_feature_names_out()}\\n')\n",
    "# summarize encoded vector\n",
    "print(f'vector shape: {vector.shape}\\n')\n",
    "print(f'article vector\\n {vector.toarray()}') \"\"\"\n",
    "\n",
    "temp_complete_test_prep = pd.DataFrame(vector.toarray(), columns=sorted(vectorizer.vocabulary_), index=temp_complete_test.index)\n",
    "print(temp_complete_test_prep.head())\n",
    "\n",
    "prep_X_train = temp_complete_test_prep[:428]\n",
    "prep_X_test = temp_complete_test_prep[428:]\n",
    "#print(prep_X_test.shape)\n",
    "#print(prep_X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import math\n",
    "\n",
    "# alpha = 1.0 ==> Laplace Smoothing; not perfectly balanced data ==> fit_prior = TRUE\n",
    "nb_classifier = MultinomialNB(alpha = 1.0, fit_prior = True)\n",
    "\n",
    "# Fit data to classifier\n",
    "nb_classifier.fit(prep_X_train_count, Y)\n",
    "#print(type(prep_X_train_count))\n",
    "\n",
    "# Get log-likelihood of features given a class, i.e. P(x_i|y)\n",
    "log_like_class0 = nb_classifier.feature_log_prob_[0]\n",
    "log_like_class1 = nb_classifier.feature_log_prob_[1]\n",
    "\n",
    "# Get indices of likelihood given class, from largest likelihood to smallest\n",
    "# By negation rule, smallest items are originally the largest\n",
    "max_index_class0 = np.argsort(-log_like_class0)\n",
    "max_index_class1 = np.argsort(-log_like_class1)\n",
    "\n",
    "# Top-20 most identifiable words over class 0\n",
    "top20_class0 = np.array(prep_X_train_count.columns)[max_index_class0][0:20]\n",
    "\n",
    "# Top-20 most identifiable words over class 1\n",
    "top20_class1 = np.array(prep_X_train_count.columns)[max_index_class1][0:20]\n",
    "\n",
    "print(top20_class0)\n",
    "print(top20_class1)\n",
    "\n",
    "# Calculate the values of the ratio between likelihoods for class 0\n",
    "# P(x_i|y = 0) / P(x_i|y = 1)\n",
    "ratio_class0 = np.exp(log_like_class0) / np.exp(log_like_class1)\n",
    "\n",
    "# Calculate the values of the ratio between likelihoods for class 1\n",
    "# P(x_i|y = 1) / P(x_i|y = 0)\n",
    "ratio_class1 = np.exp(log_like_class1) / np.exp(log_like_class0)\n",
    "\n",
    "# Sort indices of ratios, from largest ratio to smallest\n",
    "# By negation rule, smallest items are originally the largest\n",
    "max_index_ratio_class0 = np.argsort(-ratio_class0)\n",
    "max_index_ratio_class1 = np.argsort(-ratio_class1)\n",
    "\n",
    "# Top-20 words that maximise the quantity\n",
    "top20_ratio_class0 = np.array(prep_X_train_count.columns)[max_index_ratio_class0][0:20]\n",
    "top20_ratio_class1 = np.array(prep_X_train_count.columns)[max_index_ratio_class1][0:20]\n",
    "\n",
    "print(top20_ratio_class0)\n",
    "print(top20_ratio_class1)\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "#                              TO BE DELETED                             #\n",
    "##########################################################################\n",
    "# Test code (Get parameters of the classifier)\n",
    "#nb_classifier.get_params(deep=True)\n",
    "\n",
    "# Test code (Prediction)\n",
    "#print(nb_classifier.predict(prep_X_test_count))\n",
    "\n",
    "# Get \n",
    "#nb_classifier.score(prep_X_test_count, Y_test)\n",
    "\n",
    "# [instance 1:[probability of 0  probability of 1]]\n",
    "# [instance 2:[probability of 0 probability of 1]]\n",
    "#print(nb_classifier.predict_proba(prep_X_test_count))\n",
    "##########################################################################\n",
    "#                              TO BE DELETED                             #\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i)\n",
    "The top-20 most identifiable words that are most likely to occur inthe articles over class ```tech``` and ```entertainment``` is:\n",
    "|Rank|tech| |Rank| entertainment |\n",
    "|:---:|:---:|---|:---:|:---:|\n",
    "|1|said||1|said|\n",
    "|2|people||2|film|\n",
    "|3|new||3|best|\n",
    "|4|mobile||4|year|\n",
    "|5|mr||5|music|\n",
    "|6|one||6|also|\n",
    "|7|also||7|us|\n",
    "|8|would||8|new|\n",
    "|9|could||9|one|\n",
    "|10|technology||10|show|\n",
    "|11|use||11|first|\n",
    "|12|users||12|awards|\n",
    "|13|net||13|tv|\n",
    "|14|software||14|last|\n",
    "|15|games||15|uk|\n",
    "|16|us||16|actor|\n",
    "|17|music||17|number|\n",
    "|18|many||18|band|\n",
    "|19|year||19|mr|\n",
    "|20|phone||20|star|\n",
    "\n",
    "Note that in the code, we can compare the likelihood on log scale because it is monotonic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii)\n",
    "The top-20 words that maximise the following quantity\n",
    "$\\begin{align*}\n",
    "\\frac{\\mathbb{P}(X_w = 1|Y = y)}{\\mathbb{P}(X_w = 1|Y \\neq y)}\n",
    "\\end{align*}$:\n",
    "\n",
    "|Rank|Y = tech| |Rank|Y = entertainment|\n",
    "|:---:|:---:|---|:---:|:---:|\n",
    "|1|users||1|actress|\n",
    "|2|software||2|singer|\n",
    "|3|microsoft||3|oscar|\n",
    "|4|mobile||4|stars|\n",
    "|5|broadband||5|aviator|\n",
    "|6|virus||6|band|\n",
    "|7|firms||7|nominated|\n",
    "|8|pc||8|rock|\n",
    "|9|spam||9|festival|\n",
    "|10|phones||10|album|\n",
    "|11|gadget||11|nominations|\n",
    "|12|net||12|charles|\n",
    "|13|consumer||13|chart|\n",
    "|14|mobiles||14|foxx|\n",
    "|15|gadgets||15|oscars|\n",
    "|16|machines||16|starring|\n",
    "|17|windows||17|singles|\n",
    "|18|technologies||18|jamie|\n",
    "|19|systems||19|swank|\n",
    "|20|pcs||20|comedy|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (ii) top-20 words that maximise the quantity is better than the (i) top-20 most identifiable words that are most likely to occur over 2 classes. It is because (i) calculate the corresponding likelihood, but not comparing the likelihood to that of other classes. We do not know whether the word will be more likely to be in class `tech` or `entertainment`. In contrast, (ii) actually calculates the relative likelihood. If the ratio is greater than 1, it means that the word will be more likely to be in the numerator class than that of the denominator class. Therefore, the lists of (ii) will better decribe the class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
